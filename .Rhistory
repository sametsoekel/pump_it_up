dtrain <- lgb.Dataset(as.matrix(train_x), label = train_y,categorical_feature = catnames)
data_file <- tempfile(fileext = ".data")
lgb.Dataset.save(dtrain, data_file)
dtrain <- lgb.Dataset(data_file)
lgb.Dataset.construct(dtrain)
model <- lgb.train(data=dtrain,
objective = "regression",
alpha = 0.1,
nrounds = 1000,
learning_rate = .1,
metric='l2')
pred <- model$predict(as.matrix(test_x))
hata <- data.frame(pred=pred,obs=test_y)
defaultSummary(hata)
model <- lgb.train(data=dtrain,
objective = "multiclass",
alpha = 0.1,
nrounds = 1000,
learning_rate = .1,
metric='l2')
model <- lgb.train(data=dtrain,
objective = "multiclass",
alpha = 0.1,
nrounds = 1000,
learning_rate = .1
)
neural_model <- nnet(shoter,target,size = 17,rang = .1,maxit =1000)
library(nnet, lib.loc = "/usr/lib/R/library")
neural_model <- nnet(shoter,target,size = 17,rang = .1,maxit =1000)
library(dplyr)
olcek <- function(x) {(x-min(x)) / (max(x) - min(x))}
must_convert<-sapply(train_x,is.factor)       # logical vector telling if a variable needs to be displayed as numeric
M2<-sapply(train_x[,must_convert],unclass)    # data.frame of all categorical variables now displayed as numeric
trainnew<-cbind(train_x[,!must_convert],M2)
trainnew <- cbind(trainnew,train_y)
trainnew <- trainnew %>%
apply(2,olcek) %>%
data.frame()
must_convert<-sapply(test_x,is.factor)       # logical vector telling if a variable needs to be displayed as numeric
M2<-sapply(test_x[,must_convert],unclass)    # data.frame of all categorical variables now displayed as numeric
testnew<-cbind(test_x[,!must_convert],M2)
testnew <- testnew %>%
apply(2,olcek) %>%
data.frame()
shoter <- trainnew %>% select(-"train_y")
target <- trainnew$train_y
neural_model <- nnet(shoter,target,size = 17,rang = .1,maxit =1000)
fittedrescaled <- neural_model$fitted.values * (max(train$construction_year) - min(train$construction_year) ) + min(train$construction_year)
hata <- data.frame(pred = fittedrescaled,obs=train_y)
defaultSummary(hata)
library(caret)
fittedrescaled <- neural_model$fitted.values * (max(train$construction_year) - min(train$construction_year) ) + min(train$construction_year)
hata <- data.frame(pred = fittedrescaled,obs=train_y)
defaultSummary(hata)
testpred <- predict(neural_model,testnew)
testpredrescaled <- testpred * (max(test$construction_year) - min(test$construction_year) ) + min(test$construction_year)
hata <- data.frame(pred = testpredrescaled,obs=test_y)
defaultSummary(hata)
library(lightgbm)
lgbmodel<-lightgbm(dtrain)
dtrain <- lgb.Dataset(as.matrix(train_x), label = train_y,categorical_feature = catnames)
data_file <- tempfile(fileext = ".data")
lgb.Dataset.save(dtrain, data_file)
dtrain <- lgb.Dataset(data_file)
lgb.Dataset.construct(dtrain)
model <- lightgbm(data=dtrain,
objective = "regression",
alpha = 0.1,
nrounds = 1000,
learning_rate = .5
)
pred <- model$predict(as.matrix(test_x))
predlgb <- model$predict(as.matrix(test_x))
hata <- data.frame(pred = predlgb,obs=test_y)
defaultSummary(hata)
hata <- data.frame(pred = testpredrescaled,obs=test_y)
defaultSummary(hata)
model <- lightgbm(data=dtrain,
objective = "regression",
alpha = 0.1,
nrounds = 2000,
learning_rate = .1
)
predlgb <- model$predict(as.matrix(test_x))
hata <- data.frame(pred = predlgb,obs=test_y)
defaultSummary(hata)
predlgb
hata <- data.frame(pred = round(predlgb),obs=test_y)
defaultSummary(hata)
hata <- data.frame(pred = round(predlgb)-1,obs=test_y)
defaultSummary(hata)
hata <- data.frame(pred = round(predlgb)+1,obs=test_y)
defaultSummary(hata)
hata <- data.frame(pred = round(predlgb),obs=test_y)
defaultSummary(hata)
hata <- data.frame(pred = round(predlgb-1),obs=test_y)
defaultSummary(hata)
hata <- data.frame(pred = round(predlgb),obs=test_y)
defaultSummary(hata)
mean(train$construction_year)
year_train
train_x <- read.csv("Data/train_x.csv", na.strings=c("NA",""),stringsAsFactors = TRUE)
train_y <- read_csv("Data/train_y.csv")
library(readr)
train_y <- read_csv("Data/train_y.csv")
train_xx <- train_x %>% select(-c("id","wpt_name","num_private","date_recorded","recorded_by","scheme_name",))
wholeset <- cbind(
train_xx,train_y %>% select(-"id")
) %>% data.frame()
View(wholeset)
wholeset$construction_year <- ifelse(wholeset$construction_year==0,NA,wholeset$construction_year)
year_train <- wholeset %>%
filter(!is.na(construction_year))
year_train<-na.omit(year_train)
year_target <- train_xx %>%
filter(is.na(construction_year)) %>%
select(-"construction_year") %>% na.omit()
year_target <- wholeset %>%
filter(is.na(construction_year)) %>%
select(-"construction_year") %>% na.omit()
df<-year_train
save.image("~/Desktop/pump_it_up/R Scripts/workspace.RData")
train_index <- createDataPartition(df$construction_year,
p = .7,
list = F,
times = 1)
train <- df[train_index,]
test <- df[-train_index,]
train_x <- read.csv("Data/train_x.csv", na.strings=c("NA",""),stringsAsFactors = TRUE)
train_y <- read_csv("Data/train_y.csv")
###### SOME VARIABLES I FOUND INSIGNIFICANT GETS OUT -- 14/09/2020 #######
train_xx <- train_x %>% select(-c("id","wpt_name","num_private","date_recorded","recorded_by","scheme_name",))
#plasteR::na.outline(train_xx) ### checked current missing values
#Freq(as.factor(train_xx$construction_year)) ### checked faulty frequencies
wholeset <- cbind(
train_xx,train_y %>% select(-"id")
) %>% data.frame()
#### Filling zero dates with NA
wholeset$construction_year <- ifelse(wholeset$construction_year==0,NA,wholeset$construction_year)
year_train <- wholeset %>%
filter(!is.na(construction_year))
year_train<-na.omit(year_train)
year_target <- wholeset %>%
filter(is.na(construction_year)) %>%
select(-"construction_year") %>% na.omit()
##### TO DO : AFTER FILLING NA CONSTRUCTION YEARS, MERGE year_target and year_train
#### fitting a year predict model is deploying in year_predict.R using year_train set
df<-year_train
set.seed(42)
train_index <- createDataPartition(df$construction_year,
p = .7,
list = F,
times = 1)
yt_train <- df[train_index,]
yt_test <- df[-train_index,]
yt_train_x <- train %>% dplyr::select(-c("construction_year"))
yt_train_y <- yt_train$construction_year
yt_train_x <- yt_train %>% dplyr::select(-c("construction_year"))
yt_test_y <- yt_test$construction_year
yt_test_x <- yt_test %>% dplyr::select(-"Salary")
yt_test_x <- yt_test %>% dplyr::select(-"construction_year")
catnames <- names(purrr::keep(yt_train_x,is.factor))
yt_train_lgb <- lgb.Dataset(as.matrix(yt_train_x), label = yt_train_y,categorical_feature = catnames)
data_file <- tempfile(fileext = ".data")
lgb.Dataset.save(yt_train_lgb, data_file)
yt_train_lgb <- lgb.Dataset(data_file)
lgb.Dataset.construct(yt_train_lgb)
View(yt_train_x)
lgbmodel <- lgb.train(data=yt_train_lgb,
objective = "regression",
alpha = 0.1,
nrounds = 1000,
learning_rate = .1
)
lgbpred <- model$predict(as.matrix(yt_test_x))
lgbpred <- lgbmodel$predict(as.matrix(yt_test_x))
hata <- data.frame(pred=pred,obs=yt_test_y)
hata <- data.frame(pred=lgbpred,obs=yt_test_y)
defaultSummary(hata)
lgbmodel <- lgb.train(data=yt_train_lgb,
objective = "regression",
alpha = 0.01,
nrounds = 2000,
learning_rate = .3
)
lgbpred <- lgbmodel$predict(as.matrix(yt_test_x))
hata <- data.frame(pred=lgbpred,obs=yt_test_y)
defaultSummary(hata)
lgbmodel <- lgb.train(data=yt_train_lgb,
objective = "regression",
alpha = 0.1,
nrounds = 500,
learning_rate = .3
)
lgbpred <- lgbmodel$predict(as.matrix(yt_test_x))
hata <- data.frame(pred=lgbpred,obs=yt_test_y)
defaultSummary(hata)
lgbmodel <- lgb.train(data=yt_train_lgb,
objective = "regression",
alpha = 0.01,
nrounds = 500,
learning_rate = .3
)
lgbpred <- lgbmodel$predict(as.matrix(yt_test_x))
hata <- data.frame(pred=lgbpred,obs=yt_test_y)
defaultSummary(hata)
lgbmodel <- lgb.train(data=yt_train_lgb,
objective = "regression",
alpha = 0.25,
nrounds = 500,
learning_rate = .3
)
lgbpred <- lgbmodel$predict(as.matrix(yt_test_x))
hata <- data.frame(pred=lgbpred,obs=yt_test_y)
defaultSummary(hata)
lgbmodel <- lgb.train(data=yt_train_lgb,
objective = "regression",
alpha = 0.1,
nrounds = 100,
learning_rate = .3
)
lgbpred <- lgbmodel$predict(as.matrix(yt_test_x))
hata <- data.frame(pred=lgbpred,obs=yt_test_y)
defaultSummary(hata)
lgbmodel <- lgb.train(data=yt_train_lgb,
objective = "regression",
alpha = 0.1,
nrounds = 300,
learning_rate = .3
)
lgbpred <- lgbmodel$predict(as.matrix(yt_test_x))
hata <- data.frame(pred=lgbpred,obs=yt_test_y)
defaultSummary(hata)
lgbmodel <- lgb.train(data=yt_train_lgb,
objective = "regression",
alpha = 0.1,
nrounds = 500,
learning_rate = .3
)
lgbpred <- lgbmodel$predict(as.matrix(yt_test_x))
hata <- data.frame(pred=lgbpred,obs=yt_test_y)
defaultSummary(hata)
lgbmodel <- lgb.train(data=yt_train_lgb,
objective = "regression",
alpha = 0.1,
nrounds = 2500,
learning_rate = .3
)
lgbpred <- lgbmodel$predict(as.matrix(yt_test_x))
hata <- data.frame(pred=lgbpred,obs=yt_test_y)
defaultSummary(hata)
lgbmodel <- lgb.train(data=yt_train_lgb,
objective = "regression",
alpha = 0.1,
nrounds = 2500,
learning_rate = .1
)
lgbpred <- lgbmodel$predict(as.matrix(yt_test_x))
hata <- data.frame(pred=lgbpred,obs=yt_test_y)
defaultSummary(hata)
lgbmodel <- lgb.train(data=yt_train_lgb,
objective = "regression",
alpha = 0.1,
nrounds = 500,
learning_rate = .1
)
lgbpred <- lgbmodel$predict(as.matrix(yt_test_x))
hata <- data.frame(pred=lgbpred,obs=yt_test_y)
defaultSummary(hata)
lgbmodel <- lgb.train(data=yt_train_lgb,
objective = "regression",
alpha = 0.1,
nrounds = 500,
learning_rate = 1
)
lgbpred <- lgbmodel$predict(as.matrix(yt_test_x))
hata <- data.frame(pred=lgbpred,obs=yt_test_y)
defaultSummary(hata)
lgbmodel <- lgb.train(data=yt_train_lgb,
objective = "regression",
alpha = 0.1,
nrounds = 5100,
learning_rate = .1
)
lgbpred <- lgbmodel$predict(as.matrix(yt_test_x))
hata <- data.frame(pred=lgbpred,obs=yt_test_y)
defaultSummary(hata)
lgbmodel <- lgb.train(data=yt_train_lgb,
objective = "regression",
alpha = 0.1,
nrounds = 100,
learning_rate = .1
)
lgbpred <- lgbmodel$predict(as.matrix(yt_test_x))
hata <- data.frame(pred=lgbpred,obs=yt_test_y)
defaultSummary(hata)
lgbmodel <- lgb.train(data=yt_train_lgb,
objective = "regression",
alpha = 0.1,
nrounds = 1000,
learning_rate = .1
)
lgbpred <- lgbmodel$predict(as.matrix(yt_test_x))
hata <- data.frame(pred=lgbpred,obs=yt_test_y)
defaultSummary(hata)
lgb.grid <- list(objective = "regression",
metric = "RMSE",
min_sum_hessian_in_leaf = 1,
feature_fraction = 0.7,
bagging_fraction = 0.7,
bagging_freq = 5,
min_data = 100,
max_bin = 50,
lambda_l1 = 8,
lambda_l2 = 1.3,
min_data_in_bin=100,
min_gain_to_split = 10,
min_data_in_leaf = 30,
is_unbalance = TRUE)
lgb.normalizedgini <- function(preds, dtrain){
actual = getinfo(dtrain, "label")
score  = NormalizedGini(preds,actual)
return(list(name = "gini", value = score, higher_better = TRUE))
}
yt_train_lgb_cv <- lgb.Dataset(as.matrix(yt_train_x), label = yt_train_y)
yt_train_lgb_cv <- lgb.Dataset(as.matrix(yt_train_x), label = yt_train_y)
data_file_cv <- tempfile(fileext = ".data")
lgb.Dataset.save(yt_train_lgb_cv, data_file_cv)
yt_train_lgb_cv <- lgb.Dataset(data_file_cv)
lgb.Dataset.construct(yt_train_lgb_cv)
lgb.model.cv <- lgb.cv(params = lgb.grid, data = yt_train_lgb_cv, learning_rate = 0.02, num_leaves = 25,
num_threads = 2 , nrounds = 7000, early_stopping_rounds = 50,
eval_freq = 20, eval = lgb.normalizedgini,
categorical_feature = catnames, nfold = 5, stratified = TRUE)
trainset_cv <- lgb.Dataset(data=as.matrix(yt_train_x), label=yt_train_y)
yt_train_lgb_cv <- lgb.Dataset(data=as.matrix(yt_train_x), label=yt_train_y)
yt_train_lgb_cv <- lgb.Dataset(data=as.matrix(yt_train_x), label=yt_train_y)
lgb.model.cv <- lgb.cv(params = lgb.grid, data = yt_train_lgb_cv, learning_rate = 0.02, num_leaves = 25,
num_threads = 2 , nrounds = 7000, early_stopping_rounds = 50,
eval_freq = 20, eval = lgb.normalizedgini,
categorical_feature = catnames, nfold = 5, stratified = TRUE)
install.packages("MLmetrics")
library(MLmetrics)
lgb.model.cv <- lgb.cv(params = lgb.grid, data = yt_train_lgb_cv, learning_rate = 0.02, num_leaves = 25,
num_threads = 2 , nrounds = 7000, early_stopping_rounds = 50,
eval_freq = 20, eval = lgb.normalizedgini,
categorical_feature = catnames, nfold = 5, stratified = TRUE)
lgb.model.cv$best_iter
lgb.model.cv$boosters
lgb.model.cv$boosters[2]
lgb.model.cv$best_score
lgb.model.cv <- lgb.cv(params = lgb.grid, data = yt_train_lgb_cv, aplha=0.01,learning_rate = 0.1, num_leaves = 25,
num_threads = 2 , nrounds = 7000, early_stopping_rounds = 50,
eval_freq = 20, eval = lgb.normalizedgini,
categorical_feature = catnames, nfold = 5, stratified = TRUE)
lgb.model.cv$best_iter
modeller<-lgb.model.cv$boosters
View(modeller)
model1 <- modeller[[1]][["booster"]]
model1$predict(as.matrix(yt_test_x))
pred <- model1$predict(as.matrix(yt_test_x))
hata <- data.frame(pred=pred,obs=yt_test_y)
defaultSummary(hata)
model2 <- modeller[[2]][["booster"]]
pred <- model2$predict(as.matrix(yt_test_x))
hata <- data.frame(pred=pred,obs=yt_test_y)
defaultSummary(hata)
model1 <- modeller[[1]][["booster"]]
pred <- model1$predict(as.matrix(yt_test_x))
hata <- data.frame(pred=pred,obs=yt_test_y)
defaultSummary(hata)
model3 <- modeller[[3]][["booster"]]
pred <- model3$predict(as.matrix(yt_test_x))
hata <- data.frame(pred=pred,obs=yt_test_y)
defaultSummary(hata)
model4 <- modeller[[4]][["booster"]]
pred <- model4$predict(as.matrix(yt_test_x))
hata <- data.frame(pred=pred,obs=yt_test_y)
defaultSummary(hata)
model5 <- modeller[[5]][["booster"]]
pred <- model5$predict(as.matrix(yt_test_x))
hata <- data.frame(pred=pred,obs=yt_test_y)
defaultSummary(hata)
best_iter <- lgb.model.cv$best_iter
lgb_final_model = lgb.train(params = lgb.grid, data = yt_train_lgb_cv, learning_rate = 0.1,
num_leaves = 25, num_threads = 2 , nrounds = best.iter,
eval_freq = 20, eval = lgb.normalizedgini,
categorical_feature = catnames)
lgb_final_model = lgb.train(params = lgb.grid, data = yt_train_lgb_cv, learning_rate = 0.1,
num_leaves = 25, num_threads = 2 , nrounds = best_iter,
eval_freq = 20, eval = lgb.normalizedgini,
categorical_feature = catnames)
pred <- lgb_final_model$predict(as.matrix(yt_test_x))
hata <- data.frame(pred=pred,obs=yt_test_y)
defaultSummary(hata)
hata
View(hata)
hata <- data.frame(pred=round(pred),obs=yt_test_y)
defaultSummary(hata)
View(hata)
lgb.grid <- list(objective = "regression",
metric = "RMSE",
min_sum_hessian_in_leaf = 1,
feature_fraction = 0.7,
bagging_fraction = 0.7,
bagging_freq = 5,
min_data = 200,
max_bin = 50,
lambda_l1 = 5,
lambda_l2 = 1,
min_data_in_bin=200,
min_gain_to_split = 5,
min_data_in_leaf = 20,
is_unbalance = TRUE)
lgb.grid <- list(objective = "regression",
metric = "RMSE",
min_sum_hessian_in_leaf = 1,
feature_fraction = 0.7,
bagging_fraction = 0.7,
bagging_freq = 5,
min_data = 200,
max_bin = 50,
lambda_l1 = 5,
lambda_l2 = 1,
min_data_in_bin=200,
min_gain_to_split = 5,
min_data_in_leaf = 20,
is_unbalance = TRUE)
lgb.normalizedgini <- function(preds, dtrain){
actual = getinfo(dtrain, "label")
score  = NormalizedGini(preds,actual)
return(list(name = "gini", value = score, higher_better = TRUE))
}
save.image("~/Desktop/pump_it_up/R Scripts/workspace.RData")
yt_train_lgb_cv <- lgb.Dataset(data=as.matrix(yt_train_x), label=yt_train_y)
lgb.model.cv <- lgb.cv(params = lgb.grid, data = yt_train_lgb_cv, aplha=0.01,learning_rate = 0.1, num_leaves = 25,
num_threads = 2 , nrounds = 3000, early_stopping_rounds = 50,
eval_freq = 100, eval = lgb.normalizedgini,
categorical_feature = catnames, nfold = 10, stratified = TRUE)
lgb.model.cv$best_iter
best_iter <- lgb.model.cv$best_iter
lgb_final_model = lgb.train(params = lgb.grid, data = yt_train_lgb_cv, learning_rate = 0.1,
num_leaves = 25, num_threads = 2 , nrounds = best_iter,
eval_freq = 20, eval = lgb.normalizedgini,
categorical_feature = catnames)
pred <- lgb_final_model$predict(as.matrix(yt_test_x))
hata <- data.frame(pred=round(pred),obs=yt_test_y)
defaultSummary(hata)
View(hata)
modeller <- lgb.model.cv$boosters
View(modeller)
pred <- lgb_final_model$predict(as.matrix(yt_train_x))
hata <- data.frame(pred=round(pred),obs=yt_train_y)
defaultSummary(hata)
lgb.grid <- list(objective = "regression",
metric = "RMSE",
min_sum_hessian_in_leaf = 1,
feature_fraction = 0.8,
bagging_fraction = 0.8,
bagging_freq = 5,
min_data = 200,
max_bin = 50,
lambda_l1 = 5,
lambda_l2 = 1,
min_data_in_bin=200,
min_gain_to_split = 5,
min_data_in_leaf = 20,
is_unbalance = TRUE)
lgb.normalizedgini <- function(preds, dtrain){
actual = getinfo(dtrain, "label")
score  = NormalizedGini(preds,actual)
return(list(name = "gini", value = score, higher_better = TRUE))
}
yt_train_lgb_cv <- lgb.Dataset(data=as.matrix(yt_train_x), label=yt_train_y)
lgb.model.cv <- lgb.cv(params = lgb.grid, data = yt_train_lgb_cv, aplha=0.01,learning_rate = 0.1, num_leaves = 25,
num_threads = 2 , nrounds = 10000, early_stopping_rounds = 50,
eval_freq = 100, eval = lgb.normalizedgini,
categorical_feature = catnames, nfold = 13, stratified = TRUE)
best_iter <- lgb.model.cv$best_iter
best_iter
lgb_final_model = lgb.train(params = lgb.grid, data = yt_train_lgb_cv, learning_rate = 0.1,
num_leaves = 25, num_threads = 2 , nrounds = best_iter,
eval_freq = 20, eval = lgb.normalizedgini,
categorical_feature = catnames)
pred <- lgb_final_model$predict(as.matrix(yt_train_x))
hata <- data.frame(pred=round(pred),obs=yt_train_y)
defaultSummary(hata)
pred <- lgb_final_model$predict(as.matrix(yt_test_x))
hata <- data.frame(pred=round(pred),obs=yt_test_y)
defaultSummary(hata)
View(hata)
median(df$construction_year)
save(lgb_final_model, file = "16 Eyl. LGBM CV (9.21).rda")
predicted_years <- lgb_final_model$predict(as.matrix(year_target))
repairedpart <- data.frame(year_target,construction_year=predicted_years)
View(repairedpart)
repairedpart <- data.frame(year_target,construction_year=round(predicted_years))
View(year_train)
yearkeeper <- year_train$construction_year
unchangedpart <- data.frame(year_train %>% select(-"construction_year"),construction_year=yearkeeper)
View(unchangedpart)
anyNA(unchangedpart)
anyNA(repairedpart)
mergedperfect <- rbind(repairedpart,unchangedpart)
View(mergedperfect)
summary(mergedperfect$amount_tsh)
save(mergedperfect,"1st_merge.csv")
save(mergedperfect,file="1st_merge.csv")
write.csv(mergedperfect,file="1st_merge.csv")
write.csv(mergedperfect,file="1st_merge.csv")
median(train_x$amount_tsh)
summary(train_x$amount_tsh)
